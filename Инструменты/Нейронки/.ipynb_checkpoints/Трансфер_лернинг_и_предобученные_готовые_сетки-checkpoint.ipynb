{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XgbimpoMZV5F"
   },
   "source": [
    "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\", width=550, height=300></p>\n",
    "\n",
    "<h3 style=\"text-align: center;\"><b>Физтех-Школа Прикладной математики и информатики (ФПМИ) МФТИ</b></h3>\n",
    "\n",
    "---\n",
    "\n",
    "<h2 style=\"text-align: center;\"><b>Transfer Learning</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-nVLftxNZV5H"
   },
   "source": [
    "Как Вы уже знаете в современных задачах обработки изображений, будь то задача обнаружения объектов, задача распознавания образов, задача (семантической) сегментации, задача классификации изображений и другие, всё чаще используют **свёрточные нейросети** (*Convolutional Neural Networks*, *CNN*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LA5ixc_nZV5H"
   },
   "source": [
    "Они показывают очень хорошие результаты, за ними стоит как [математический аппарат](https://stats.stackexchange.com/questions/269854/are-there-mathematical-reasons-for-convolution-in-neural-networks-beyond-expedie), так и эвристики, полученные опытным путём."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUH-Q7FuZV5I"
   },
   "source": [
    "В данном задании Вам предстоит познакомиться с архитектурами *AlexNet*, *VGG* и *Inception* и для каждой из этих моделей использовать технику **Transfer Learning**.  \n",
    "\n",
    "* **Transfer Learning** - это процесс дообучения на **новых данных** какой-либо нейросети, уже обученной до этого на других данных, обычно на каком-нибудь хорошем, большом (миллионы картинок) датасете (например, [ImageNet](http://www.image-net.org/) ~ 14 млн картинок)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaiyKxA7Sgvf"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>AlexNet</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7nFB2hnISgvj"
   },
   "source": [
    "**AlexNet** - нейронная сеть, которая победила в ILSVRC (соревнование по классификации картинок из ImageNet) в 2012 году и стала основой для многих других архитектур. Впервые она была представлена в статье  “ImageNet Classification with Deep Convolutional Neural Networks”, над которой работал Джоффри Хинтон - человек, которого многие называют отцом современного computer vision.\n",
    "\n",
    "Архитектура описана на картинке ниже"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-Hj_RLcSgvm"
   },
   "source": [
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2018/05/AlexNet-1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4zmFA6QSgvq"
   },
   "source": [
    "**AlexNet** состоит из 5 **сверточных** слоев, 3 **MaxPool** слоев и 2 **FullyConnected** слоев в конце. Обратите внимание, что в последнем пулинг слое окна, из которых берется максимум, пересекаются за счет того, что *stride*=2. Это изменение по сравнению с традиционным пулингом помогло снизить ошибку на 0.4%.\n",
    "\n",
    "По сути **AlexNet** это самая базовая архитектура для сверточной сети после LeNet, которую мы уже писали на предыдущем занятии."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5jQNFS5Sgvv"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>VGG</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPhsK5U2Sgvy"
   },
   "source": [
    "Один **сверточный** слой с фильтром 5$\\times$5 можно заменить двумя подряд идущими слоями с фильтрами размером 3$\\times$3, так как **воспринимаемая область** картинки у них будет одинаковой. При этом уменьшиться количество параметров, поэтому такую сеть будет легче обучать. \n",
    "\n",
    "На момент создания VGG люди уже заметили, что чем больше слоев в нейросети, тем выше ее точность. Заменяя большие фильтры на несколько фильтров 3$\\times$3 исследователи получили глубокую нейросеть с меньшим количеством параметров. Архитектура VGG-16 (версии VGG с 16 слоями) представлена на картинке ниже:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5Fwng6XSgv3"
   },
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1040/1*0Tk4JclhGOCR_uLe6RKvUQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8naUWATeSgv6"
   },
   "source": [
    "Когда говорят **VGG**, то чаще всего имеют ввиду **VGG-16** или **VGG-19**. Более глубоких версий **VGG** нет, так как после 19 слоев точность начинает падать.\n",
    "\n",
    "Чтобы добиться высоких результатов в соревновании при обучении и валидации нейросети использовались дополнительные премы, подробнее о которых можно прочитать в [статье на Medium](https://medium.com/coinmonks/paper-review-of-vggnet-1st-runner-up-of-ilsvlc-2014-image-classification-d02355543a11)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EAjLsFn2Sgv8"
   },
   "source": [
    "\n",
    "<h2 style=\"text-align: center;\"><b>Inception v1</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVLsNug8SgwB"
   },
   "source": [
    "### Рассмотрим идею, которая подтолкнула исследователей к созданию этой архитектуры.\n",
    "\n",
    "Площадь, которую занимает классифицируемый объект, может очень сильно отличаться. Пример на картинке ниже: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prERHawrSgwD"
   },
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1040/1*aBdPBGAeta-_AM4aEyqeTQ.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoPk-Gs7SgwF"
   },
   "source": [
    "* Для извлечения информации с большой площади лучше всего подходят **большие** фильтры, и наоборот для маленьких объектов лучше **маленькие** фильтры. \n",
    "* Глубокие нейронные сети намного сложнее обучать: в них появляется проблема **затухания градиента** и они **переобучаются**.\n",
    "Чтобы решить первую проблему исследователи придумали **Incepton** модуль, который применяет фильтры разного размера и затем склеивает полученные каналы. При этом извлекается как информация из больших объектов, так и из маленьких. Простейшая реализация модуля выглядит так:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1040/1*DKjGRDd_lJeUfVlY50ojOA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhUHlhcdSgwJ"
   },
   "source": [
    "Реализацию можно сделать более эффективной, если сначала уменьшить количество каналов с помощью **сверточного слоя** 1$\\times$1 и лишь затем применить **слой** с фильтрами 5$\\times$5. Сокращение вычислений происходит за счет того, что мы сначала **уменьшаем размерность** данных и лишь затем преобразовываем их. Продвинутая реализация:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1040/1*U_McJnp7Fnif-lw9iIC5Bw.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYUeTuXWSgwL"
   },
   "source": [
    "Сеть состоит из **корня** (нескольких сверточных слоев) и **Inception** модулей идущих за ним. Оранжевым прямоугольников выделен корень, а фиолетовыми - **вспомогательные классификаторы**. Именно они помогают бороться со второй проблемой, которую мы упомянули ранее. Наша функция потерь - взвешенная сумма **LogLoss** на двух **вспомогательных классификаторах** и **основном** в конце нейронной сети.\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1040/1*uW81y16b-ptBDV8SIT1beQ.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mkJw0QkcSgwN"
   },
   "source": [
    "После Inception v1 были представлены 2, 3 и 4 версии, пррочитать о которых вы можете  в [статье на Medium](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202).\n",
    "\n",
    "Однако сейчас научились иначе бороться с затуханием градиентов с помощью **residual conncection**. Это позволило увеличить число слоев в нейронной сети.\n",
    "\n",
    "![](https://i.imgur.com/XwcnU5x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bx9ZdMmMZV5I"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Transfer Learning</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j-EtKIoMZV5J"
   },
   "source": [
    "Теперь мы перейдем к тому, как можно использовать уже обученные нейросети, чтобы ускорить свою работу.\n",
    "\n",
    "Давайте вспомним общую архитектуру CNN:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWER66eyZV5K"
   },
   "source": [
    "<img src=\"https://drive.google.com/uc?id=14pApKqQjnmWMXazY0HHjREn9rI9uwCQg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bihUjSwGZV5L"
   },
   "source": [
    "С помощью операций *свёртки (convolution)* и *пулинга (pooling)* всё, что расположено до этапа *classification*, по сути **извлекает признаки из объектов, подающихся на вход** (картинок, в данном случае). То есть вместо того, чтобы самим пытаться как-то описать картинки для хорошей работы классификатора, мы предоставляем заняться этим нейросети (обучая её методом обратного распространения ошибки ([лекция 4](https://www.youtube.com/watch?v=HZDOhHAg5_g)))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gi9ad_suZV5L"
   },
   "source": [
    "**Вопрос (творческий):**  А какие признаки для картинок приходят Вам в голову? (считать, что картинки цветные (3 канала), все одинакового размера)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AMX8k3uZV5M"
   },
   "source": [
    "**Ответ:** <Ваши мысли>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H5JCuydFZV5O"
   },
   "source": [
    "Представим теперь, что eсть свой набор данных, и Вы хотите научить сеть классифицировать объекты из Вашей выборки.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEoe0qi_ZV5O"
   },
   "source": [
    "Надеемся, что теперь Вам стало понятнее, как обучать крутые сети на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU8jKks8ZV5P"
   },
   "source": [
    "<h2 style=\"text-align: center;\"><b>Переходим к практике</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyvXMfhZZV5Q"
   },
   "source": [
    "<p style=\"text-align: center;\"><i>(основано на http://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html)</i></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XFlfXQsZV5R"
   },
   "source": [
    "Мы будем пользоваться библиотекой PyTorch. Если Вы её ещё не установили, то вот [инструкция на Wiki по установке PyTorch](https://github.com/deepmipt/dlschl/wiki/%D0%98%D0%BD%D1%81%D1%82%D1%80%D1%83%D0%BA%D1%86%D0%B8%D1%8F-%D0%BF%D0%BE-%D1%83%D1%81%D1%82%D0%B0%D0%BD%D0%BE%D0%B2%D0%BA%D0%B5-PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xF5zpbFgkIp",
    "outputId": "b9e07517-98ab-44ce-f281-02244e3e7a7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 22.0.3; however, version 22.1.1 is available.\n",
      "You should consider upgrading via the 'C:\\ProgramData\\Anaconda3\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q torchvision catalyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l2sTUHmNZV5S"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weT3x9DQZV5X"
   },
   "source": [
    "### В чём состоит задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F8paFz_5ZV5Y"
   },
   "source": [
    "Вам предстоит попробовать использовать  типа архитектур свёрточных нейросетей - **AlexNet (сделано за Вас в примере), VGG16, Inception_v3** - как *Feature Extractor*, с помощью *Fine Tuning* и *\"из коробки\"*. \n",
    "\n",
    "**Для каждого пункта нужно:**\n",
    "- вывести график loss'а на обучающей и на валидационной выборке\n",
    "- вывести качество модели (accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99LAkQNVZV5Y"
   },
   "source": [
    "### Данные  \n",
    "\n",
    "В данном задании используются сети (из библиотеки **torchvision**), предобученные на датасете ImageNet.  \n",
    "В качестве новых данных будет датасет Меравьи vs Пчёлы, Вам нужно скачать его отсюда: **[Муравьи vs Пчёлы](https://download.pytorch.org/tutorial/hymenoptera_data.zip)**, *являющийся частью датасета ImageNet*. В нём 400 картинок, ~250 обучение и ~150 валидация (тест)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gGQA6CbZV5Z"
   },
   "source": [
    "### Функции для отрисовки и обучения модели:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LnoL3STNZV5a"
   },
   "source": [
    "* Загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v5iKptyqo7hv",
    "outputId": "22b02dcb-bbd7-4336-f286-67d6afc3a4d9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"wget\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n",
      "\"unzip\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "!wget https://download.pytorch.org/tutorial/hymenoptera_data.zip\n",
    "!unzip hymenoptera_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "1lKZZSUYZV5b"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Системе не удается найти указанный путь: './hymenoptera_data\\\\train'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7820/1869741192.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# папка с данными. Если запускаете в колабе, нужно скопировать данные к себе в директорию и примонтировать диск. Если запускаете локально -- просто скачайте данные\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./hymenoptera_data'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\u001b[0m\u001b[0;32m     20\u001b[0m                                           data_transforms[x])\n\u001b[0;32m     21\u001b[0m                   for x in ['train', 'val']}\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7820/1869741192.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# папка с данными. Если запускаете в колабе, нужно скопировать данные к себе в директорию и примонтировать диск. Если запускаете локально -- просто скачайте данные\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mdata_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./hymenoptera_data'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n\u001b[0m\u001b[0;32m     20\u001b[0m                                           data_transforms[x])\n\u001b[0;32m     21\u001b[0m                   for x in ['train', 'val']}\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     ):\n\u001b[1;32m--> 310\u001b[1;33m         super().__init__(\n\u001b[0m\u001b[0;32m    311\u001b[0m             \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m             \u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[0;32m    143\u001b[0m     ) -> None:\n\u001b[0;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m         \u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;33m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m         \"\"\"\n\u001b[1;32m--> 219\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torchvision\\datasets\\folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \"\"\"\n\u001b[1;32m---> 41\u001b[1;33m     \u001b[0mclasses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Системе не удается найти указанный путь: './hymenoptera_data\\\\train'"
     ]
    }
   ],
   "source": [
    "# Преобразование обучающих данных для расширения обучающей выборки и её нормализация\n",
    "# Для валидационной (тестовой) выборки только нормализация\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(244),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(244),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "# папка с данными. Если запускаете в колабе, нужно скопировать данные к себе в директорию и примонтировать диск. Если запускаете локально -- просто скачайте данные\n",
    "data_dir = './hymenoptera_data'\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "# специальный класс для загрузки данных в виде батчей\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    image_datasets[\"train\"], batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    image_datasets[\"val\"], batch_size=128\n",
    ")\n",
    "\n",
    "loaders = {\n",
    "    \"train\": train_dataloader,\n",
    "    \"valid\": val_dataloader\n",
    "}\n",
    "\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = image_datasets['train'].classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03FqarW9ZV5e"
   },
   "source": [
    "Размеры обучающей и валидационной выборок:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxjRd-5OZV5f",
    "outputId": "03ee327d-6b0d-4ef3-f717-6e7504ed1c69"
   },
   "outputs": [],
   "source": [
    "print(dataset_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vx3QiNAcZV5k"
   },
   "source": [
    "**Вопрос (на понимание кода выше):**  \n",
    "1. В DataLoader() выше стоит \"shuffle=True\". Для чего это нужно?\n",
    "2. Сколько картинок будет в каждом батче?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZPe4pfoZV5l"
   },
   "source": [
    "**Ответ:** <Ваш ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXqEVA1ZZV5m"
   },
   "source": [
    "* Посмотрим на картинки из датасета:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "id": "wQG2_RrvZV5n",
    "outputId": "e3171ca1-9a6e-4519-c606-b7f3fadc5e11"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)\n",
    "\n",
    "\n",
    "# Получим 1 батч (картнки-метки) из обучающей выборки\n",
    "inputs, classes = next(iter(loaders['train']))\n",
    "\n",
    "# Расположим картинки рядом\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KqOOeBFpZV6T",
    "outputId": "82f9857a-253b-4dfe-cd54-865ba40fb8bc"
   },
   "outputs": [],
   "source": [
    "image_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6exRIPRbSgzS"
   },
   "source": [
    "### Обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzP9AdAwZV5s"
   },
   "source": [
    "Следующая функция будет использоваться для обучения модели. Аргументы:  \n",
    "* model $-$ нейросеть\n",
    "* loss $-$ оптимизируемая функция (criterion, cost function, objective)\n",
    "* optimizer $-$ оптимизационный алгоритм\n",
    "* scheduler $-$ политика изменения learning_rate\n",
    "* num_epochs $-$ количество итераций обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QkMmJ7W-pjtA"
   },
   "outputs": [],
   "source": [
    "from catalyst import dl, metrics\n",
    "\n",
    "class ImgRunner(dl.Runner):\n",
    "\n",
    "    def predict_batch(self, batch):\n",
    "        # model inference step\n",
    "        return self.model(batch[0].to(self.device).view(batch[0].size(0), -1))\n",
    "\n",
    "    def _handle_batch(self, batch):\n",
    "        # model train/valid step\n",
    "        x, y = batch\n",
    "        self.input = {\"targets\": y}\n",
    "        y_hat = self.model(x)\n",
    "        self.output = {\"logits\": y_hat}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Afrq3SA1ZV53"
   },
   "source": [
    "### Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNCYHC8oZV53"
   },
   "source": [
    "Для каждой из следующих нейросетей:\n",
    "* **AlexNet** (уже сделано в примере)\n",
    "* **VGG16**\n",
    "* **Inception_v3**\n",
    "\n",
    "Напишите код и выведите результат (график лосса, accuracy и вывод примера классификации картинок с визализацией (с помощью функции `vizualize_model()`)) для трёх способов:\n",
    "* Использование готовой нейросети **\"из коробки\"**\n",
    "* Использование нейросети как **Feature Extractor**\n",
    "* **Fine Tuning** нейросети\n",
    "\n",
    "Для каждого пункта нужно:\n",
    "* сделать с сетью то, что нужно в пункте (\"из коробки\", FE или FT)\n",
    "* вывести график loss'а на обучающей и на валидационной выборке\n",
    "* вывести качество модели (accuracy) на валидационной (тестовой) выборке\n",
    "* (по желанию) использовать функцию visualize_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8XzWmgE7ZV54"
   },
   "source": [
    "### AlexNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g6zW24cSZV56"
   },
   "source": [
    "*ПРИМЕЧАНИЕ: Здесь не выведены графики loss'а и не использована visualize_model(). От Вас это ожидается.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmLx0a_uZV56"
   },
   "source": [
    "Загрузка модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82,
     "referenced_widgets": [
      "20b73bd4fd7c4eabb0d60e3cffa0f40e",
      "343f05ba41844c63bf5a1c8233bf4227",
      "f22575d8fff24b0ebc9fa2953ffdf014",
      "3aa04a4e0eac4f908a964c8630f6f961",
      "b1c9e384b5db4d5381f74c6a8edee642",
      "67126c76f02e4ef1bfde60ae33380daf",
      "344e4e6aaee04801b5ca70ba113d7885",
      "28fefac7987f4a4a8de68294e7b01289"
     ]
    },
    "id": "1mHbrvXAZV57",
    "outputId": "7726f6ab-1423-4c4f-9064-6a6677cdf8a3"
   },
   "outputs": [],
   "source": [
    "model = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_g9XqLsZV5-"
   },
   "source": [
    "Посмотрим, что внутри:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N-OLqslsZV6A",
    "outputId": "cb0f61f7-d47a-4863-bc6b-b8dc04214482"
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O9LYMESWZV6E"
   },
   "source": [
    "Видим, что на вход классификатору (classifier) подаётся *9216 признаков*. Это и будет размер входа для нашего нового классификатора."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA5qB523ZV6G"
   },
   "source": [
    "* **Fine Tuning** способ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxucpA3FZV6H"
   },
   "source": [
    "Сконфигурируем - изменим FC-слой и зададим *cost function* и *оптимизирующий алгоритм*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JKPZYgVZV6J"
   },
   "source": [
    "(*по умолчанию backpropagation распространяется на все слои, поэтому здесь мы только заменяем FC-слой на свой классификатор*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGw8dN6QwWHI"
   },
   "outputs": [],
   "source": [
    "!rm -rf logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppIGmkz4ZV6K"
   },
   "outputs": [],
   "source": [
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 4096\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model.classifier[6] = nn.Linear(num_features, 2)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_MsDT40aSg1n"
   },
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DXDZK7JVZV6X",
    "outputId": "f8734f9e-3a19-4095-f6e7-cb38ed8b6045",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner = ImgRunner()\n",
    "runner.train(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    callbacks=[\n",
    "        dl.CriterionCallback(),\n",
    "        dl.OptimizerCallback(), \n",
    "        dl.AccuracyCallback()\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    main_metric=\"accuracy01\",\n",
    "    minimize_metric=False,\n",
    "    logdir=\"logs/alexnet\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vyM6SLNZZV6g"
   },
   "source": [
    "* **Feature Extractor** способ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "81XVFTJiZV6h"
   },
   "outputs": [],
   "source": [
    "model_extractor = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6XUzgLSlZV6m"
   },
   "source": [
    "Помним, что по-умолчанию все слои нейросети обучаются заново:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kiGUDNqZV6n",
    "outputId": "c9c074d7-c4fd-4c54-f7b1-17901118d8ab"
   },
   "outputs": [],
   "source": [
    "for param in model_extractor.parameters():\n",
    "    print(param.requires_grad)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOTXa28JZV6s"
   },
   "source": [
    "Сделаем так, чтобы на них *не распространялся backpropagation* (заморозим их), и подменим классификатор (ведь старый уже с весами для ImageNet'а)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7rW4vKaZV6u"
   },
   "outputs": [],
   "source": [
    "# замораживаем параметры (веса)\n",
    "for param in model_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 4096\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model_extractor.classifier[6] = nn.Linear(num_features, 2)\n",
    "\n",
    "# Обучаем только классификатор\n",
    "optimizer = optim.Adam(model_extractor.classifier[6].parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bVL7TjRIZV6z",
    "outputId": "0233d46a-c3f5-495b-ecb1-ca82a711f8e5"
   },
   "outputs": [],
   "source": [
    "runner = ImgRunner()\n",
    "\n",
    "runner.train(\n",
    "    model=model_extractor,\n",
    "    optimizer=optimizer,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    callbacks=[\n",
    "        dl.CriterionCallback(),\n",
    "        dl.OptimizerCallback(), \n",
    "        dl.AccuracyCallback()\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    main_metric=\"accuracy01\",\n",
    "    minimize_metric=False,\n",
    "    logdir=\"logs/alexnet_freeze\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RmehLHv9Sg3d"
   },
   "source": [
    "* **Смешанный** способ:\n",
    "Мы будем обучать не только последний **fully connected** слой, но и несколько предпоследних"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NAcI23-OSg3e"
   },
   "outputs": [],
   "source": [
    "model_mixed = models.alexnet(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "69YQJd2hzzi7",
    "outputId": "fb79fa59-e1ae-45cf-d23c-4775870895e6"
   },
   "outputs": [],
   "source": [
    "model_mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TG7n2z9bSg3h"
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "layers_to_unfreeze = 5\n",
    "\n",
    "# Выключаем подсчет градиентов для слоев, которые не будем обучать\n",
    "for param in model_mixed.features[:-layers_to_unfreeze].parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 4096\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model_mixed.classifier[6] = nn.Linear(num_features, 2)\n",
    "\n",
    "# Обучаем последние layers_to_unfreeze слоев из сверточной части и fully connected слой \n",
    "# parameters() возвращает просто список тензоров парамтеров, поэтому два таких списка можно сложить\n",
    "optimizer = torch.optim.Adam(\n",
    "    chain(\n",
    "        list(model_mixed.features.parameters())[-layers_to_unfreeze:],\n",
    "        model_mixed.classifier.parameters()\n",
    "    ),\n",
    "    lr=1e-4, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "908lTwBoSg3n",
    "outputId": "6ae0f226-c86a-4201-b041-d915b0b157ac",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner = ImgRunner()\n",
    "\n",
    "runner.train(\n",
    "    model=model_mixed,\n",
    "    optimizer=optimizer,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    callbacks=[\n",
    "        dl.CriterionCallback(),\n",
    "        dl.OptimizerCallback(), \n",
    "        dl.AccuracyCallback()\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    main_metric=\"accuracy01\",\n",
    "    minimize_metric=False,\n",
    "    logdir=\"logs/alexnet_mixed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ue3eslBDZV67"
   },
   "source": [
    "**Вопрос 1 (важный):** С чем связано повышение качества если мы перестаем учить всю сеть? (Подсказка: посмотрите на датасет и на то, как он согласуется с 4-мя ситуациями, описанными выше)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zz5G8Z2IZV69"
   },
   "source": [
    "**Ответ (важный):** <Ваш ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXmlLczGSg4l"
   },
   "source": [
    "**Вопрос 2**: Почему разморозка последних слоев не дает прироста к точности, хотя разморозить несколько послдних слоев обычно хорошеее решение для классификации похожего датасета? (Вопрос на внимательность)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bA2MxfZfSg4q"
   },
   "source": [
    "**Ответ:** <Ваш ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3GXxu9u4Ssj"
   },
   "source": [
    "### Бонус\n",
    "\n",
    "Существует еще один интересный способ. Мы не хотим, чтобы ядра в свертках сильно менялись во время обучения, а еще мы знаем, что чем меньше lr, тем меньше изменения. Давайте уменьшим lr на feature extractor-е! Разберемся как это сделать..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyphERHE5SER"
   },
   "outputs": [],
   "source": [
    "model_mixed_lr = models.alexnet(pretrained=True)\n",
    "\n",
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 4096\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model_mixed.classifier[6] = nn.Linear(num_features, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-yJuZbE5fLT"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "    (\n",
    "        {\n",
    "            \"params\": model_mixed_lr.features.parameters(),\n",
    "            \"lr\": 1e-6,\n",
    "        },\n",
    "        {\n",
    "            \"params\": model_mixed_lr.classifier.parameters(),\n",
    "        }\n",
    "     ),\n",
    "     lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8jQOU2sP6qYf",
    "outputId": "6943dea6-118c-48e5-fab1-2c2080551dc3"
   },
   "outputs": [],
   "source": [
    "runner = ImgRunner()\n",
    "\n",
    "runner.train(\n",
    "    model=model_mixed_lr,\n",
    "    optimizer=optimizer,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    callbacks=[\n",
    "        dl.CriterionCallback(),\n",
    "        dl.OptimizerCallback(), \n",
    "        dl.AccuracyCallback()\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    main_metric=\"accuracy01\",\n",
    "    minimize_metric=False,\n",
    "    logdir=\"logs/alexnet_mixed_lr\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90JYIt39ZV7W"
   },
   "source": [
    "### Другие, более современные нейросети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6McVYGoIZV7X"
   },
   "source": [
    "**Вопрос:** Какую стратегию Вы выберете, учитывая размер и специфику нового датасета?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gg9c_ngBZV7X"
   },
   "source": [
    "**Ответ:** <Ваш ответ>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLb_VQ-nZV7Y"
   },
   "source": [
    "### ResNet 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "7a2e2f74a33240f4a3339b372ef9186c",
      "0870977eef884fe5ad692d9029c82cbe",
      "8f98f2cf615f4e90a1f598b3925d62ee",
      "f84bed6f81db48e5beed044daf3c88b4",
      "04f46984f3e94abbac4daa36f2e8bd6f",
      "b16fff1c7af641c9aec0577a73d3291e",
      "6d502c18c105499eb2f95d90b5ad93d1",
      "a68968be963d4ad68d3ee3919ec7cf3c"
     ]
    },
    "id": "UQDYaJdJ6iPf",
    "outputId": "ae3167fb-78da-4ae1-c9d8-52e13b73d2de"
   },
   "outputs": [],
   "source": [
    "models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrmyWuu-ZV7Z"
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь\n",
    "model_extractor = models.resnet18(pretrained=True)\n",
    "\n",
    "# замораживаем параметры (веса)\n",
    "for param in model_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# num_features -- это размерность вектора фич, поступающего на вход FC-слою\n",
    "num_features = 512\n",
    "# Заменяем Fully-Connected слой на наш линейный классификатор\n",
    "model_extractor.fc = nn.Linear(num_features, 2)\n",
    "\n",
    "# Обучаем только классификатор\n",
    "optimizer = optim.Adam(model_extractor.fc.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5b72ftgCSg44",
    "outputId": "484de814-ffb5-422c-e559-c0665c53d041",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runner = ImgRunner()\n",
    "\n",
    "runner.train(\n",
    "    model=model_extractor,\n",
    "    optimizer=optimizer,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    callbacks=[\n",
    "        dl.CriterionCallback(),\n",
    "        dl.OptimizerCallback(), \n",
    "        dl.AccuracyCallback()\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    main_metric=\"accuracy01\",\n",
    "    minimize_metric=False,\n",
    "    logdir=\"logs/resnet\",\n",
    "    load_best_on_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrgVXFdY9-Ls",
    "outputId": "42e37615-6f00-44d7-d363-81d547c4ae17"
   },
   "outputs": [],
   "source": [
    "# замораживаем параметры (веса)\n",
    "for param in model_extractor.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "optimizer = optim.Adam(model_extractor.parameters(), lr=1e-4)\n",
    "\n",
    "runner = ImgRunner()\n",
    "\n",
    "runner.train(\n",
    "    model=model_extractor,\n",
    "    optimizer=optimizer,\n",
    "    criterion=nn.CrossEntropyLoss(),\n",
    "    callbacks=[\n",
    "        dl.CriterionCallback(),\n",
    "        dl.OptimizerCallback(), \n",
    "        dl.AccuracyCallback()\n",
    "    ],\n",
    "    loaders=loaders,\n",
    "    num_epochs=10,\n",
    "    verbose=True,\n",
    "    main_metric=\"accuracy01\",\n",
    "    minimize_metric=False,\n",
    "    logdir=\"logs/resnet_step2\",\n",
    "    load_best_on_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c57p6EIi6qv2"
   },
   "source": [
    "\n",
    "<h2 style=\"text-align: center;\"><b>Полезные ссылки</b></h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CvOZffV6qv2"
   },
   "source": [
    "1). *cs231n: http://cs231n.github.io/transfer-learning/*\n",
    "\n",
    "2). *Туториал на PyTorch Tutorials: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial*\n",
    "\n",
    "3). *Статья на Medium про TL в PyTorch: https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4*  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "99LAkQNVZV5Y"
   ],
   "name": "[seminar]transfer_learning.ipynb\"",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04f46984f3e94abbac4daa36f2e8bd6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "0870977eef884fe5ad692d9029c82cbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "20b73bd4fd7c4eabb0d60e3cffa0f40e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f22575d8fff24b0ebc9fa2953ffdf014",
       "IPY_MODEL_3aa04a4e0eac4f908a964c8630f6f961"
      ],
      "layout": "IPY_MODEL_343f05ba41844c63bf5a1c8233bf4227"
     }
    },
    "28fefac7987f4a4a8de68294e7b01289": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "343f05ba41844c63bf5a1c8233bf4227": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "344e4e6aaee04801b5ca70ba113d7885": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3aa04a4e0eac4f908a964c8630f6f961": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28fefac7987f4a4a8de68294e7b01289",
      "placeholder": "​",
      "style": "IPY_MODEL_344e4e6aaee04801b5ca70ba113d7885",
      "value": " 233M/233M [00:10&lt;00:00, 22.3MB/s]"
     }
    },
    "67126c76f02e4ef1bfde60ae33380daf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6d502c18c105499eb2f95d90b5ad93d1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a2e2f74a33240f4a3339b372ef9186c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8f98f2cf615f4e90a1f598b3925d62ee",
       "IPY_MODEL_f84bed6f81db48e5beed044daf3c88b4"
      ],
      "layout": "IPY_MODEL_0870977eef884fe5ad692d9029c82cbe"
     }
    },
    "8f98f2cf615f4e90a1f598b3925d62ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b16fff1c7af641c9aec0577a73d3291e",
      "max": 46827520,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_04f46984f3e94abbac4daa36f2e8bd6f",
      "value": 46827520
     }
    },
    "a68968be963d4ad68d3ee3919ec7cf3c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b16fff1c7af641c9aec0577a73d3291e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1c9e384b5db4d5381f74c6a8edee642": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "f22575d8fff24b0ebc9fa2953ffdf014": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67126c76f02e4ef1bfde60ae33380daf",
      "max": 244418560,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b1c9e384b5db4d5381f74c6a8edee642",
      "value": 244418560
     }
    },
    "f84bed6f81db48e5beed044daf3c88b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a68968be963d4ad68d3ee3919ec7cf3c",
      "placeholder": "​",
      "style": "IPY_MODEL_6d502c18c105499eb2f95d90b5ad93d1",
      "value": " 44.7M/44.7M [00:00&lt;00:00, 148MB/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
